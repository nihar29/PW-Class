{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
        "example of each.\n",
        "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
        "a given dataset?\n",
        "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
        "a real-world scenario.\n",
        "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
        "address this issue?\n",
        "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
        "regression? In what situations would you prefer to use polynomial regression?"
      ],
      "metadata": {
        "id": "StADY2EV0uph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1: Difference between Simple Linear Regression and Multiple Linear Regression**\n",
        "- **Simple Linear Regression**: Involves one independent variable (predictor) and one dependent variable (response).  \n",
        "  **Example**: Predicting house price based on square footage.  \n",
        "\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1x + \\epsilon\n",
        "  \\]\n",
        "\n",
        "  where \\( y \\) is the house price, \\( x \\) is the square footage, \\( \\beta_0 \\) is the intercept, \\( \\beta_1 \\) is the slope, and \\( \\epsilon \\) is the error term.\n",
        "\n",
        "- **Multiple Linear Regression**: Involves two or more independent variables.  \n",
        "  **Example**: Predicting house price based on square footage, number of bedrooms, and location.\n",
        "\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n + \\epsilon\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2: Assumptions of Linear Regression and How to Check Them**\n",
        "1. **Linearity**: The relationship between independent and dependent variables should be linear.  \n",
        "   **Check**: Scatter plots, residual plots.  \n",
        "2. **Independence**: Observations should be independent.  \n",
        "   **Check**: Durbin-Watson test.  \n",
        "3. **Homoscedasticity**: Constant variance of residuals.  \n",
        "   **Check**: Residual plot (should have constant spread).  \n",
        "4. **Normality**: Residuals should be normally distributed.  \n",
        "   **Check**: Histogram, Q-Q plot, Shapiro-Wilk test.  \n",
        "5. **No Multicollinearity**: Independent variables should not be highly correlated.  \n",
        "   **Check**: Variance Inflation Factor (VIF).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q3: Interpretation of Slope and Intercept**\n",
        "- **Intercept (\\(\\beta_0\\))**: The predicted value of \\( y \\) when all independent variables are zero.  \n",
        "- **Slope (\\(\\beta_1\\))**: The change in \\( y \\) for a one-unit increase in \\( x \\).  \n",
        "\n",
        "**Example**:  \n",
        "If a model predicts **salary** based on **years of experience**:\n",
        "\n",
        "\\[\n",
        "Salary = 30,000 + 5,000 \\times (Years\\ of\\ Experience)\n",
        "\\]\n",
        "\n",
        "- **Intercept (30,000)**: Base salary when experience = 0.  \n",
        "- **Slope (5,000)**: Each additional year of experience increases salary by â‚¹5,000.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q4: Gradient Descent in Machine Learning**\n",
        "Gradient descent is an optimization algorithm used to minimize the error in machine learning models by iteratively adjusting model parameters.  \n",
        "1. Start with random values of parameters (\\(\\beta_0, \\beta_1, \\dots\\)).  \n",
        "2. Compute the cost function (Mean Squared Error).  \n",
        "3. Compute the gradient (partial derivatives).  \n",
        "4. Update parameters using:\n",
        "\n",
        "   \\[\n",
        "   \\theta_j := \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
        "   \\]\n",
        "\n",
        "   where \\( \\alpha \\) is the learning rate.  \n",
        "5. Repeat until convergence.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q5: Multiple Linear Regression Model**\n",
        "- It models a dependent variable based on **multiple independent variables**.  \n",
        "- Equation:\n",
        "\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n + \\epsilon\n",
        "  \\]\n",
        "\n",
        "- **Difference from Simple Linear Regression**: It has more than one predictor variable, allowing for more complex relationships.  \n",
        "\n",
        "**Example**: Predicting car price based on **age, mileage, and brand**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q6: Multicollinearity in Multiple Linear Regression**\n",
        "**Definition**: When two or more independent variables are highly correlated, making it difficult to determine individual effects.  \n",
        "\n",
        "**How to Detect?**  \n",
        "1. **Correlation Matrix**: High correlations (\\(|r| > 0.7\\)) indicate multicollinearity.  \n",
        "2. **Variance Inflation Factor (VIF)**: VIF > 5 suggests multicollinearity.  \n",
        "\n",
        "**How to Fix?**  \n",
        "1. Remove highly correlated variables.  \n",
        "2. Use **Principal Component Analysis (PCA)**.  \n",
        "3. Use **Ridge Regression** or **Lasso Regression**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q7: Polynomial Regression Model**\n",
        "Polynomial regression extends linear regression by adding polynomial terms to capture non-linear relationships.\n",
        "\n",
        "Equation:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_nx^n + \\epsilon\n",
        "\\]\n",
        "\n",
        "**Difference from Linear Regression**:  \n",
        "- Linear regression fits a **straight line**.  \n",
        "- Polynomial regression fits a **curved line** (quadratic, cubic, etc.).  \n",
        "\n",
        "**Example**: Predicting house prices where the effect of size is non-linear.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q8: Advantages and Disadvantages of Polynomial Regression**\n",
        "**Advantages**:  \n",
        "Captures non-linear relationships.  \n",
        "More flexible than simple linear regression.  \n",
        "\n",
        "**Disadvantages**:  \n",
        "Prone to overfitting for high-degree polynomials.  \n",
        "More complex and computationally expensive.  \n",
        "\n",
        "**When to Use?**  \n",
        " If the relationship between variables is non-linear.  \n",
        " When linear regression gives high error.  \n",
        "Use **cross-validation** to check overfitting.  \n"
      ],
      "metadata": {
        "id": "onoBqfU1052y"
      }
    }
  ]
}