{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. What is Bayes' Theorem?**  \n",
        "Bayes' Theorem is a mathematical formula used to determine the probability of a hypothesis given prior knowledge. It helps update probabilities based on new evidence.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. Formula for Bayes' Theorem**  \n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
        "\\]\n",
        "Where:  \n",
        "- \\( P(A|B) \\) = Posterior probability (probability of \\( A \\) given \\( B \\))  \n",
        "- \\( P(B|A) \\) = Likelihood (probability of \\( B \\) given \\( A \\))  \n",
        "- \\( P(A) \\) = Prior probability of \\( A \\)  \n",
        "- \\( P(B) \\) = Marginal probability of \\( B \\)  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. Practical Uses of Bayes' Theorem**  \n",
        "- **Spam filtering** (determining if an email is spam or not).  \n",
        "- **Medical diagnosis** (probability of having a disease given test results).  \n",
        "- **Weather prediction** (updating forecasts based on new conditions).  \n",
        "- **Fraud detection** (determining if a transaction is fraudulent based on past data).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. Relationship Between Bayes' Theorem and Conditional Probability**  \n",
        "Bayes' Theorem is derived from conditional probability and is used to **reverse the probability** direction.  \n",
        "Conditional probability gives \\( P(A|B) \\) directly, while Bayes' Theorem helps compute \\( P(A|B) \\) when only \\( P(B|A) \\) is known.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. Choosing the Right Type of Naive Bayes Classifier**  \n",
        "1. **Gaussian Naive Bayes** → Used when features are **continuous** and assumed to be normally distributed.  \n",
        "   *Example: Predicting student grades based on continuous scores.*  \n",
        "2. **Multinomial Naive Bayes** → Used for **discrete** data, such as word frequencies in text classification.  \n",
        "   *Example: Spam filtering in emails.*  \n",
        "3. **Bernoulli Naive Bayes** → Used when features are **binary** (0 or 1).  \n",
        "   *Example: Sentiment analysis where presence/absence of words matters.*  \n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. Assignment: Classifying a New Instance Using Naive Bayes**  \n",
        "#### **Step 1: Compute Prior Probabilities**  \n",
        "Since equal priors are given:  \n",
        "\\[\n",
        "P(A) = P(B) = 0.5\n",
        "\\]\n",
        "\n",
        "#### **Step 2: Compute Likelihood Probabilities for \\( X_1 = 3 \\) and \\( X_2 = 4 \\)**  \n",
        "Using the frequency table:  \n",
        "\\[\n",
        "P(X_1 = 3 | A) = \\frac{4}{(3+3+4)} = \\frac{4}{10} = 0.4\n",
        "\\]\n",
        "\\[\n",
        "P(X_2 = 4 | A) = \\frac{3}{(4+3+3+3)} = \\frac{3}{13}\n",
        "\\]\n",
        "\\[\n",
        "P(X_1 = 3 | B) = \\frac{1}{(2+2+1)} = \\frac{1}{5} = 0.2\n",
        "\\]\n",
        "\\[\n",
        "P(X_2 = 4 | B) = \\frac{3}{(2+2+2+3)} = \\frac{3}{9} = 0.33\n",
        "\\]\n",
        "\n",
        "#### **Step 3: Compute Posterior Probabilities Using Bayes' Theorem**  \n",
        "\\[\n",
        "P(A | X_1=3, X_2=4) \\propto P(X_1=3 | A) \\cdot P(X_2=4 | A) \\cdot P(A)\n",
        "\\]\n",
        "\\[\n",
        "= 0.4 \\times \\frac{3}{13} \\times 0.5 = 0.046\n",
        "\\]\n",
        "\\[\n",
        "P(B | X_1=3, X_2=4) \\propto P(X_1=3 | B) \\cdot P(X_2=4 | B) \\cdot P(B)\n",
        "\\]\n",
        "\\[\n",
        "= 0.2 \\times 0.33 \\times 0.5 = 0.033\n",
        "\\]\n",
        "\n",
        "#### **Step 4: Predict the Class**  \n",
        "Since \\( P(A | X_1=3, X_2=4) > P(B | X_1=3, X_2=4) \\), the model predicts **Class A**."
      ],
      "metadata": {
        "id": "VVX6BVzcbaTc"
      }
    }
  ]
}