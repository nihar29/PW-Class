{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the Filter method in feature selection, and how does it work?\n",
        "Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
        "Q3. What are some common techniques used in Embedded feature selection methods?\n",
        "Q4. What are some drawbacks of using the Filter method for feature selection?\n",
        "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
        "selection?\n",
        "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
        "You are unsure of which features to include in the model because the dataset contains several different\n",
        "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n",
        "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
        "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
        "method to select the most relevant features for the model.\n",
        "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
        "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
        "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
        "predictor."
      ],
      "metadata": {
        "id": "FRnPmwnzzdur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:**  \n",
        "The Filter method in feature selection evaluates the relevance of features based on statistical metrics before training a model. It ranks features using methods like correlation coefficients, chi-square tests, or mutual information, selecting only the most relevant ones. It is independent of any specific model and works well with large datasets.  \n",
        "\n",
        "**Q2:**  \n",
        "The Wrapper method differs from the Filter method as it selects features based on the performance of a specific predictive model. It repeatedly trains and evaluates the model with different feature subsets using techniques like forward selection, backward elimination, or recursive feature elimination, making it more computationally expensive but often more accurate.  \n",
        "\n",
        "**Q3:**  \n",
        "Common techniques in Embedded feature selection include Lasso (L1 regularization), which shrinks less important feature coefficients to zero, Decision Tree-based methods like feature importance in Random Forests, and Gradient Boosting techniques like XGBoost’s feature selection. These methods integrate feature selection during model training.  \n",
        "\n",
        "**Q4:**  \n",
        "The Filter method's main drawbacks include ignoring feature interactions and being model-agnostic, which may not always align with a specific model’s performance. It may also select irrelevant features if the statistical metric used does not accurately capture their importance for prediction.  \n",
        "\n",
        "**Q5:**  \n",
        "The Filter method is preferred when dealing with high-dimensional data, as it is computationally efficient. It is useful in scenarios where speed is crucial, such as preprocessing large datasets in text classification or gene selection in bioinformatics, where domain knowledge can guide feature selection.  \n",
        "\n",
        "**Q6:**  \n",
        "To use the Filter Method in customer churn prediction, I would first compute statistical scores like correlation coefficients, chi-square values, or information gain for each feature with respect to the churn variable. Features with high relevance scores would be retained, while low-scoring or redundant features would be removed to improve model efficiency and accuracy.  \n",
        "\n",
        "**Q7:**  \n",
        "For predicting soccer match outcomes using the Embedded method, I would use a model like Lasso regression or tree-based algorithms such as Random Forest. These models rank feature importance during training, allowing me to select the most relevant player statistics and team rankings while discarding less impactful features.  \n",
        "\n",
        "**Q8:**  \n",
        "In house price prediction using the Wrapper method, I would iteratively train the model with different subsets of features, using techniques like forward selection or backward elimination. The best subset would be determined based on performance metrics like RMSE or R² on a validation set, ensuring optimal feature selection for accurate price prediction."
      ],
      "metadata": {
        "id": "WnkRYaaMzelW"
      }
    }
  ]
}