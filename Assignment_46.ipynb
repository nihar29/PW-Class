{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
        "represent?\n",
        "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
        "Q3. When is it more appropriate to use adjusted R-squared?\n",
        "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
        "calculated, and what do they represent?\n",
        "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
        "regression analysis.\n",
        "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
        "it more appropriate to use?\n",
        "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
        "example to illustrate.\n",
        "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
        "choice for regression analysis.\n",
        "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
        "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
        "performer, and why? Are there any limitations to your choice of metric?\n",
        "Q10. You are comparing the performance of two regularized linear models using different types of\n",
        "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
        "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
        "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
        "method?"
      ],
      "metadata": {
        "id": "JBdRsfUf3ls5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1: R-squared in Linear Regression**\n",
        "- **Definition**: R-squared (\\( R^2 \\)) measures the proportion of variance in the dependent variable that is explained by the independent variables in the model.\n",
        "- **Formula**:\n",
        "\n",
        "  \\[\n",
        "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "  \\]\n",
        "\n",
        "  where \\( y_i \\) are actual values, \\( \\hat{y}_i \\) are predicted values, and \\( \\bar{y} \\) is the mean of \\( y \\).\n",
        "\n",
        "- **Interpretation**:  \n",
        "  - \\( R^2 = 0 \\) → Model explains no variance.  \n",
        "  - \\( R^2 = 1 \\) → Model perfectly fits the data.  \n",
        "  - Higher \\( R^2 \\) indicates a better fit but does not imply causation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2: Adjusted R-squared vs. Regular R-squared**\n",
        "- **Adjusted R-squared** adjusts for the number of predictors in the model to prevent overestimation of model performance.\n",
        "\n",
        "  \\[\n",
        "  R^2_{adj} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n",
        "  \\]\n",
        "\n",
        "  where \\( n \\) is the number of observations, and \\( p \\) is the number of predictors.\n",
        "\n",
        "- **Difference**:\n",
        "  - **Regular \\( R^2 \\)** increases with more predictors, even if they are irrelevant.\n",
        "  - **Adjusted \\( R^2 \\)** only increases if new predictors improve the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3: When to Use Adjusted R-squared?**\n",
        "- When comparing models with different numbers of predictors.\n",
        "- When adding new features to check if they truly improve the model.\n",
        "- When avoiding overfitting due to excessive predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4: RMSE, MSE, and MAE in Regression Analysis**\n",
        "- **Mean Absolute Error (MAE)**: Average absolute difference between actual and predicted values.\n",
        "\n",
        "  \\[\n",
        "  MAE = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|\n",
        "  \\]\n",
        "\n",
        "- **Mean Squared Error (MSE)**: Average squared difference between actual and predicted values.\n",
        "\n",
        "  \\[\n",
        "  MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
        "  \\]\n",
        "\n",
        "- **Root Mean Squared Error (RMSE)**: Square root of MSE, measuring error in the original units.\n",
        "\n",
        "  \\[\n",
        "  RMSE = \\sqrt{MSE}\n",
        "  \\]\n",
        "\n",
        "- **Interpretation**:\n",
        "  - MAE is less sensitive to large errors.\n",
        "  - MSE and RMSE penalize large errors more heavily.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5: Advantages and Disadvantages of RMSE, MSE, and MAE**\n",
        "| Metric  | Advantages  | Disadvantages  |\n",
        "|---------|------------|---------------|\n",
        "| **MAE**  | Easy to interpret | Treats all errors equally (ignores variance). |\n",
        "| **MSE**  | Penalizes large errors more, useful for outliers | Not in original units, difficult to interpret. |\n",
        "| **RMSE**  | Same units as the dependent variable, useful for model comparison | More sensitive to large errors, can be misleading with skewed data. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6: Lasso Regularization vs. Ridge Regularization**\n",
        "- **Lasso (Least Absolute Shrinkage and Selection Operator)**:\n",
        "  - Adds **L1 penalty**:\n",
        "\n",
        "    \\[\n",
        "    \\sum |\\beta_j|\n",
        "    \\]\n",
        "\n",
        "  - Shrinks coefficients and can set some to **zero** (feature selection).\n",
        "  - Useful when many irrelevant features exist.\n",
        "\n",
        "- **Ridge Regularization**:\n",
        "  - Adds **L2 penalty**:\n",
        "\n",
        "    \\[\n",
        "    \\sum \\beta_j^2\n",
        "    \\]\n",
        "\n",
        "  - Shrinks coefficients but **does not** set them to zero.\n",
        "  - Useful when features are correlated.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7: How Regularized Models Prevent Overfitting**\n",
        "- **Regularization (Lasso/Ridge)** reduces overfitting by penalizing large coefficients.\n",
        "- **Example**:\n",
        "  - Without regularization, a model with many features may fit noise.\n",
        "  - Ridge keeps all features but reduces their impact.\n",
        "  - Lasso removes irrelevant features, making the model simpler.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q8: Limitations of Regularized Models**\n",
        "- **Lasso**: May remove important features if lambda is too high.\n",
        "- **Ridge**: Does not perform feature selection, making interpretation difficult.\n",
        "- **Both**: Regularization strength (\\(\\lambda\\)) must be carefully tuned.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q9: Comparing Model A (RMSE = 10) and Model B (MAE = 8)**\n",
        "- RMSE penalizes larger errors more than MAE.\n",
        "- If large errors are rare but impactful, **Model A (RMSE = 10)** may be worse.\n",
        "- If errors are generally small and evenly spread, **Model B (MAE = 8)** is better.\n",
        "- **Limitations**: MAE and RMSE are not directly comparable; RMSE exaggerates large errors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q10: Comparing Ridge (0.1) vs. Lasso (0.5)**\n",
        "- **Lasso (\\(\\lambda = 0.5\\))** may remove some features entirely.\n",
        "- **Ridge (\\(\\lambda = 0.1\\))** keeps all features but shrinks them.\n",
        "- **Choice**:\n",
        "  - If feature selection is needed → **Lasso**.\n",
        "  - If correlated features exist → **Ridge**.\n",
        "  - **Trade-offs**: Higher \\(\\lambda\\) increases bias but reduces overfitting.\n"
      ],
      "metadata": {
        "id": "mMsD4YVI3m5-"
      }
    }
  ]
}