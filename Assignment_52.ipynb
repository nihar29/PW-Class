{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is the purpose of Grid Search CV in machine learning, and how does it work?  \n",
        "Grid Search CV is used for hyperparameter tuning in machine learning models. It works by exhaustively searching through a predefined set of hyperparameters and evaluating each combination using cross-validation. This helps in selecting the best hyperparameters that optimize model performance.  \n",
        "\n",
        "### Q2. Describe the difference between Grid Search CV and Randomized Search CV, and when might you choose one over the other?  \n",
        "**Grid Search CV** tests all possible hyperparameter combinations, making it computationally expensive but thorough. **Randomized Search CV** randomly selects a subset of hyperparameters, making it faster and suitable when the search space is large or time is limited.  \n",
        "\n",
        "### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.  \n",
        "Data leakage occurs when information from outside the training dataset improperly influences the model, leading to artificially high accuracy but poor real-world performance. Example: Including future stock prices as a feature when predicting stock trends.  \n",
        "\n",
        "### Q4. How can you prevent data leakage when building a machine learning model?  \n",
        "To prevent data leakage, ensure proper train-test splitting, apply feature engineering (e.g., scaling) only on training data, and avoid using target-related information in feature selection. Cross-validation should be correctly implemented to prevent information from leaking between folds.  \n",
        "\n",
        "### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?  \n",
        "A confusion matrix is a table that summarizes the performance of a classification model by showing **True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN)**. It helps in understanding model accuracy, precision, recall, and overall classification effectiveness.  \n",
        "\n",
        "### Q6. Explain the difference between precision and recall in the context of a confusion matrix.  \n",
        "**Precision** = TP / (TP + FP) measures how many predicted positives are actually correct.  \n",
        "**Recall** = TP / (TP + FN) measures how many actual positives are correctly identified.  \n",
        "Precision is important when false positives are costly, while recall is crucial when missing actual positives is critical.  \n",
        "\n",
        "### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?  \n",
        "By analyzing the values in the confusion matrix, you can determine whether the model is making more **false positives (incorrectly classifying negatives as positives)** or **false negatives (missing actual positives)**. This helps in deciding whether to focus on improving precision or recall.  \n",
        "\n",
        "### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?  \n",
        "- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)  \n",
        "- **Precision** = TP / (TP + FP)  \n",
        "- **Recall** = TP / (TP + FN)  \n",
        "- **F1-Score** = 2 × (Precision × Recall) / (Precision + Recall)  \n",
        "- **Specificity** = TN / (TN + FP)  \n",
        "\n",
        "### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?  \n",
        "Accuracy depends on the ratio of correct predictions (TP + TN) to total samples. However, in imbalanced datasets, accuracy can be misleading as a model may classify most cases as the majority class while ignoring minority class errors, making precision and recall more informative.  \n",
        "\n",
        "### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?  \n",
        "If a model consistently misclassifies a particular class (e.g., high false negatives for a minority group), it may indicate bias. Disproportionate errors in different categories suggest the need for class balancing, re-weighting, or improved feature selection to address performance gaps."
      ],
      "metadata": {
        "id": "ISaRMRXrEEUa"
      }
    }
  ]
}