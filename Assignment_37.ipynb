{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
        "can they be mitigated?\n",
        "Q2: How can we reduce overfitting? Explain in brief.\n",
        "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
        "variance, and how do they affect model performance?\n",
        "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
        "How can you determine whether your model is overfitting or underfitting?\n",
        "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
        "and high variance models, and how do they differ in terms of their performance?Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
        "some common regularization techniques and how they work."
      ],
      "metadata": {
        "id": "O__YFr0Hx2UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:**  \n",
        "Overfitting occurs when a machine learning model learns the noise in the training data instead of the actual pattern, leading to poor generalization on new data. Underfitting happens when the model is too simple to capture the underlying patterns, resulting in low accuracy. Overfitting can be mitigated using regularization, pruning, and more training data, while underfitting can be reduced by increasing model complexity or using better features.  \n",
        "\n",
        "**Q2:**  \n",
        "Overfitting can be reduced using techniques such as regularization (L1/L2), increasing training data, early stopping, dropout in neural networks, cross-validation, and reducing model complexity. These methods ensure that the model generalizes well to unseen data.  \n",
        "\n",
        "**Q3:**  \n",
        "Underfitting occurs when a model is too simple to learn patterns in the data, leading to poor performance on both training and test data. Scenarios where underfitting occurs include using a linear regression model on non-linear data, training deep learning models with very few epochs, or selecting insufficient features for a complex problem.  \n",
        "\n",
        "**Q4:**  \n",
        "The bias-variance tradeoff is a fundamental concept in ML that balances a model’s ability to generalize. High bias (underfitting) means the model is too simple, missing patterns, while high variance (overfitting) means the model learns noise, making it too sensitive to small fluctuations in data. The goal is to find an optimal balance to minimize total error.  \n",
        "\n",
        "**Q5:**  \n",
        "To detect overfitting, compare training and validation accuracy—if training accuracy is high but validation accuracy is low, overfitting is likely. For underfitting, if both training and validation accuracies are low, the model is too simple. Techniques like learning curves, cross-validation, and testing on unseen data help diagnose these issues.  \n",
        "\n",
        "**Q6:**  \n",
        "Bias refers to systematic errors due to overly simplistic models, while variance refers to sensitivity to small fluctuations in data. High bias models (e.g., linear regression on complex data) perform poorly on both training and test sets. High variance models (e.g., deep neural networks with small datasets) perform well on training data but fail on new data. The key is finding a balance for good generalization.  \n",
        "\n",
        "**Q7:**  \n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. Common methods include L1 regularization (Lasso), which shrinks some feature coefficients to zero, and L2 regularization (Ridge), which prevents extreme weights. Dropout in neural networks randomly removes neurons during training, improving generalization."
      ],
      "metadata": {
        "id": "YzqEfWtFyQ11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vel2GBZOyIqF"
      }
    }
  ]
}