{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.  \n",
        "Linear regression predicts continuous values by fitting a straight line to the data, while logistic regression predicts probabilities for categorical outcomes (e.g., binary classification) using the sigmoid function. Logistic regression would be more appropriate for scenarios like **spam email classification**, where the goal is to predict whether an email is spam (1) or not spam (0).  \n",
        "\n",
        "### Q2. What is the cost function used in logistic regression, and how is it optimized?  \n",
        "Logistic regression uses the **log loss (binary cross-entropy) cost function**, which measures how well the predicted probabilities match actual labels. It is optimized using **gradient descent**, which iteratively updates the model parameters to minimize the loss.  \n",
        "\n",
        "### Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.  \n",
        "Regularization adds a penalty term to the cost function to prevent overfitting by controlling large coefficient values. **L1 regularization (Lasso)** can shrink some coefficients to zero, effectively performing feature selection, while **L2 regularization (Ridge)** prevents large coefficients but retains all features. **Elastic Net** combines both for balanced regularization.  \n",
        "\n",
        "### Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?  \n",
        "The **ROC (Receiver Operating Characteristic) curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at different threshold values. A model with an area under the curve (**AUC-ROC**) close to 1 indicates good performance, while a value near 0.5 suggests random guessing.  \n",
        "\n",
        "### Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?  \n",
        "Feature selection techniques include **L1 regularization (Lasso)**, **Recursive Feature Elimination (RFE)**, **Mutual Information**, and **Chi-Square test**. These methods help remove irrelevant or redundant features, improving model interpretability and reducing overfitting.  \n",
        "\n",
        "### Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?  \n",
        "Strategies for handling imbalance include **resampling techniques** (oversampling the minority class with SMOTE or undersampling the majority class), **using weighted loss functions** (giving higher weight to the minority class), and **threshold tuning** to optimize precision-recall balance.  \n",
        "\n",
        "### Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?  \n",
        "Common challenges include **multicollinearity** (which can be addressed using **VIF analysis** or **L2 regularization**), **overfitting** (handled through **regularization and feature selection**), and **poor convergence** (fixed by **scaling data and adjusting learning rates**). Addressing these issues improves model stability and interpretability."
      ],
      "metadata": {
        "id": "KE5xrYX8Ddgc"
      }
    }
  ]
}