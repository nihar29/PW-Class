{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Describe the decision tree classifier algorithm and how it works to make predictions.  \n",
        "A **decision tree classifier** is a supervised learning algorithm that splits the dataset into smaller subsets based on feature values to make predictions. It builds a tree structure where each internal node represents a decision on a feature, each branch represents an outcome, and each leaf node represents a class label. Predictions are made by traversing the tree from the root node to a leaf based on feature values.  \n",
        "\n",
        "### Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.  \n",
        "1. **Select the Best Feature** – Use measures like **Gini Impurity** or **Entropy** to find the best feature to split on.  \n",
        "2. **Calculate Impurity** –  \n",
        "   - Entropy: \\( H(S) = -\\sum p_i \\log_2 p_i \\)  \n",
        "   - Gini Impurity: \\( G = 1 - \\sum p_i^2 \\)  \n",
        "3. **Perform Splitting** – The dataset is divided based on the selected feature’s threshold value.  \n",
        "4. **Repeat for Subsets** – Steps 1-3 are applied recursively until stopping conditions (e.g., max depth, minimum samples per leaf) are met.  \n",
        "5. **Assign Labels** – Leaf nodes get the majority class from their respective subsets.  \n",
        "\n",
        "### Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.  \n",
        "A decision tree classifier solves binary classification problems by recursively splitting the dataset into two subsets at each node based on feature values. At each step, the algorithm selects the best feature that minimizes impurity (entropy/Gini). The process continues until stopping criteria are met, and class labels (0 or 1) are assigned to leaf nodes. New instances are classified by following the decision path from root to leaf.  \n",
        "\n",
        "### Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.  \n",
        "Geometrically, a decision tree **partitions the feature space into rectangular regions**. Each split creates decision boundaries that divide data into distinct classes. For example, in a 2D space with two features, each split creates vertical or horizontal cuts, forming axis-aligned regions. A new point is classified by checking which region it falls into, determining its class label.  \n",
        "\n",
        "### Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.  \n",
        "A **confusion matrix** is a table that summarizes the performance of a classification model by comparing predicted labels with actual labels. It has four components:  \n",
        "- **True Positives (TP):** Correctly predicted positives  \n",
        "- **True Negatives (TN):** Correctly predicted negatives  \n",
        "- **False Positives (FP):** Incorrectly predicted positives  \n",
        "- **False Negatives (FN):** Incorrectly predicted negatives  \n",
        "It helps compute evaluation metrics like **accuracy, precision, recall, and F1-score** to assess model performance.  \n",
        "\n",
        "### Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.  \n",
        "Example confusion matrix for a binary classification problem:  \n",
        "\n",
        "| Actual \\ Predicted | Positive (1) | Negative (0) |  \n",
        "|--------------------|-------------|-------------|  \n",
        "| Positive (1)      | **50 (TP)**  | **10 (FN)** |  \n",
        "| Negative (0)      | **5 (FP)**   | **100 (TN)** |  \n",
        "\n",
        "- **Precision** = \\( \\frac{TP}{TP + FP} = \\frac{50}{50+5} = 0.91 \\)  \n",
        "- **Recall** = \\( \\frac{TP}{TP + FN} = \\frac{50}{50+10} = 0.83 \\)  \n",
        "- **F1-score** = \\( 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = 2 \\times \\frac{0.91 \\times 0.83}{0.91 + 0.83} = 0.87 \\)  \n",
        "\n",
        "### Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.  \n",
        "Choosing the right metric depends on the problem’s context:  \n",
        "- **Accuracy** is suitable for balanced datasets.  \n",
        "- **Precision** is important when false positives are costly (e.g., spam filtering).  \n",
        "- **Recall** is crucial when missing positives is risky (e.g., medical diagnosis).  \n",
        "- **F1-score** is useful for imbalanced datasets as it balances precision and recall.  \n",
        "- **ROC-AUC** is beneficial for ranking models.  \n",
        "\n",
        "### Q8. Provide an example of a classification problem where precision is the most important metric, and explain why.  \n",
        "In **spam email detection**, precision is more important because a false positive (classifying a legitimate email as spam) is more harmful than a false negative (missing a spam email). High precision ensures that flagged emails are truly spam, reducing the risk of losing important messages.  \n",
        "\n",
        "### Q9. Provide an example of a classification problem where recall is the most important metric, and explain why.  \n",
        "In **cancer diagnosis**, recall is crucial because missing a positive case (false negative) can lead to severe consequences. A model with high recall ensures that most cancer patients are correctly identified, even if it means some healthy individuals are misclassified as positive (false positives)."
      ],
      "metadata": {
        "id": "YWxIr_loKeys"
      }
    }
  ]
}